{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning101.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeDAu1Uz4wJTTDr5NQoAEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedhere/MLPNS2021/blob/main/NNDL/deeplearning101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC3F9gAKEzMy"
      },
      "source": [
        "Creating a multipayer perceptron from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h07rHMTBbQn"
      },
      "source": [
        "from https://iamtrask.github.io/2015/07/12/basic-python-network/ with minor modifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AooF2lpsBa-w"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH_3xM9WBasf"
      },
      "source": [
        "# trivial NN for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzsBqIVMBYSD"
      },
      "source": [
        "X = np.array([ [0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1] ])\n",
        "\n",
        "y = np.array([[0,1,1,0]]).T\n",
        "print(\"predict:\\n\", y)\n",
        "print(\"based on:\\n\", X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N6F7CHYDwi-"
      },
      "source": [
        "import pylab as pl\n",
        "%pylab inline\n",
        "\n",
        "pl.imshow(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg3_Mpt9Cgdt"
      },
      "source": [
        "# architecture: 1 layer\n",
        "\n",
        "input layer 4x3: observations 3 features\n",
        "\n",
        "neurons layer: 3x1\n",
        "\n",
        "$[4\\times3] \\cdot [3\\times1] => [1]$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MciCn2aUdSy2"
      },
      "source": [
        "#define the activation function\n",
        "def sigmoid(x):\n",
        "  return 1. / (1 + np.exp(-x))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFy5MlGBdScu"
      },
      "source": [
        "\n",
        "#define the derivative of the activation function\n",
        "def dsigmoid(x):\n",
        "  return x * (1 - x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b7W4BgXdSND"
      },
      "source": [
        "# define the loss function (L1)\n",
        "def loss(prediction):\n",
        "    return y - prediction\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kp5s1LXEfVV"
      },
      "source": [
        "training = 6000\n",
        "# build the synopsis: THE WEIGHTS, the initial guess of the weights. \n",
        "# we have no knowledge, so we just randomize it\n",
        "\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "\n",
        "# initialize weights randomly with mean 0\n",
        "syn0 = 2*np.random.random((3,1)) - 1\n",
        "print (syn0.shape)\n",
        "\n",
        "for iter in range(training):\n",
        "\n",
        "   # forward propagation step\n",
        "    inputLayer = X\n",
        "    #dot product\n",
        "    l1 = np.dot(inputLayer, syn0)\n",
        "    #activate\n",
        "    output = sigmoid(l1)\n",
        "\n",
        "    # how much did we miss by?\n",
        "    output_error = loss(output)\n",
        "    #back propagation step\n",
        "    # multiply how much we missed by the\n",
        "    # slope of the sigmoid at the values of L1  \n",
        "    output_delta = output_error * dsigmoid(output)\n",
        "    # how much did we miss?\n",
        " \n",
        "    # update weights\n",
        "    syn0 += np.dot(inputLayer.T, output_delta)\n",
        "print (\"Output After Training:\\n\", output)\n",
        "print (\"target: \\n\", y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-rTqPdsEHQ3"
      },
      "source": [
        "# architecture: 2 layer\n",
        "\n",
        "input layer 4x3: observations 3 features\n",
        "\n",
        "neurons layer: 3x4, 4x1\n",
        "\n",
        "$[4\\times3] \\cdot [3\\times4] \\cdot [4\\times1] => [1]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-g4_5fKHgan"
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "# randomly initialize our weights with mean 0\n",
        "syn0 = 2 * np.random.random((3, 4)) - 1\n",
        "syn1 = 2 * np.random.random((4, 1)) - 1\n",
        "\n",
        "print (syn0.shape, \"\\n\", syn1.shape)\n",
        "\n",
        "loss_hidden = []\n",
        "loss_output = []\n",
        "\n",
        "# 2 layers\n",
        "for iterate in range(training):\n",
        "  inputLayer = X  \n",
        "  #dot product\n",
        "  la1 = np.dot(inputLayer, syn0)\n",
        "  #activate\n",
        "  layer1out = sigmoid(la1)\n",
        "\n",
        "  la2 = np.dot(layer1out, syn1)\n",
        "  #activate\n",
        "  outputLayer = sigmoid(la2)\n",
        "  \n",
        "  #calculate loss on the output layer\n",
        "  outputLayer_error = loss(outputLayer)\n",
        "  loss_output.append(outputLayer_error.sum())\n",
        "  outputLayer_delta = outputLayer_error * dsigmoid(outputLayer)\n",
        "\n",
        "  l1_error = outputLayer_delta.dot(syn1.T)\n",
        "  loss_hidden.append(l1_error.sum())\n",
        "  l1_delta = l1_error * dsigmoid(layer1out)\n",
        " \n",
        "  # back propagation step\n",
        "  # multiply how much we missed by the\n",
        "  # slope of the sigmoid at the values in l1\n",
        "\n",
        "  syn1 += outputLayer.T.dot(outputLayer_delta)\n",
        "  syn0 += inputLayer.T.dot(l1_delta)\n",
        "  \n",
        "print (\"Final Prediction:\\n\", outputLayer)\n",
        "\n",
        "print (\"target: \\n\", y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ig1tuBWS9b"
      },
      "source": [
        "pl.plot(loss_output, label=\"output\")\n",
        "pl.plot(loss_hidden, label=\"hidden\")\n",
        "pl.legend()\n",
        "pl.xlabel(\"iteration\")\n",
        "pl.ylabel(\"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFqoz3MSrIcJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}